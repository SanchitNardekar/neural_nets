{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f218f12-0377-4984-a549-facc360c465e",
   "metadata": {},
   "source": [
    "# Building a Neural Language Model \n",
    "Following [Bengio et al 2003](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4589b645-e28c-4826-99f8-26d704341238",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fa8751-8e8d-4f4f-b61c-0643a05c8890",
   "metadata": {},
   "source": [
    "The architecture of the MLP we'll be building in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d27f265-f766-4326-9f7d-fe58d0f5fd6f",
   "metadata": {},
   "source": [
    "![hello](neural_probabalistic_language_model_architecture.png \"MLP Architecture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "882ab511",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "624907cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de9e059d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e6f5c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef44f704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n",
      "... ---> e\n",
      "..e ---> m\n",
      ".em ---> m\n",
      "emm ---> a\n",
      "mma ---> .\n",
      "olivia\n",
      "... ---> o\n",
      "..o ---> l\n",
      ".ol ---> i\n",
      "oli ---> v\n",
      "liv ---> i\n",
      "ivi ---> a\n",
      "via ---> .\n",
      "ava\n",
      "... ---> a\n",
      "..a ---> v\n",
      ".av ---> a\n",
      "ava ---> .\n",
      "isabella\n",
      "... ---> i\n",
      "..i ---> s\n",
      ".is ---> a\n",
      "isa ---> b\n",
      "sab ---> e\n",
      "abe ---> l\n",
      "bel ---> l\n",
      "ell ---> a\n",
      "lla ---> .\n",
      "sophia\n",
      "... ---> s\n",
      "..s ---> o\n",
      ".so ---> p\n",
      "sop ---> h\n",
      "oph ---> i\n",
      "phi ---> a\n",
      "hia ---> .\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "\n",
    "block_size = 3\n",
    "X, Y = [], []\n",
    "\n",
    "for w in words[:5]:\n",
    "    print(w)\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "        context = context[1:] + [ix] # crop and append\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3095c3c-2329-475c-8fce-da2d0d8f7712",
   "metadata": {},
   "source": [
    "From just 5 words we can create a dataset of 32 examples using `.` as padding characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "952ab04d-5987-4135-828a-0d964376866e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.int64, torch.Size([32]), torch.int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dtype, Y.shape, Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04564df4-bdda-412e-bb74-88b6f4f11ef5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1212,  0.1038],\n",
       "        [-1.6709,  0.5939],\n",
       "        [-0.2411, -0.7757],\n",
       "        [ 0.1091, -0.4565],\n",
       "        [ 1.2677, -2.0133],\n",
       "        [ 1.2921,  0.2726],\n",
       "        [-0.0924, -0.9406],\n",
       "        [-0.6139,  0.4221],\n",
       "        [-0.8019, -1.5593],\n",
       "        [ 0.8555,  0.2140],\n",
       "        [ 0.7285,  0.1717],\n",
       "        [-1.6290,  0.5480],\n",
       "        [ 0.1451,  2.2464],\n",
       "        [-2.8325,  1.4847],\n",
       "        [-0.0165,  0.5479],\n",
       "        [-0.6087,  1.4239],\n",
       "        [-0.1148, -1.6994],\n",
       "        [ 0.5736, -0.7081],\n",
       "        [ 1.4550,  1.6645],\n",
       "        [ 0.0562, -2.2055],\n",
       "        [-0.7893,  0.7932],\n",
       "        [ 0.9540, -0.0100],\n",
       "        [ 0.0269, -0.8091],\n",
       "        [-0.1257, -0.5199],\n",
       "        [-0.6768, -0.0090],\n",
       "        [ 0.4176, -0.7839],\n",
       "        [-0.0574,  0.1648]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We now initialise the Tensor for each character in our vocabulary (27) and it's corresponding embedding.\n",
    "# For now in 2D as we only have 27 characters\n",
    "# In Bengio et al, they had 17,000 words and embedded in as small as a 30 dimensional space\n",
    "C = torch.randn((27, 2))\n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7072904e-85d4-41eb-9703-98c8e1cd2876",
   "metadata": {},
   "source": [
    "Let's try and embed a single integer first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1eb3b635-6243-422e-b9a6-55bdac37314e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.2921, 0.2726])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for 5:\n",
    "C[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "985b8c19-bfdd-4a66-b777-bab89f229888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using one-hot ecoding\n",
    "# F.one_hot(5, num_classes=27) # Doesn't work, needs to be a tensor\n",
    "F.one_hot(torch.tensor(5), num_classes=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "040e6f37-9040-4b6e-b815-745d3d26ec44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doesn't work since the onehot encoded \n",
    "# F.one_hot(torch.tensor(5), num_classes=27) @ C\n",
    "# RuntimeError: expected m1 and m2 to have the same dtype, but got: long long != float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d593eb74-7e0a-46cf-b665-3aad91249c38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.2921, 0.2726])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PyTorch needs to have a float Tensor to know how to multiply with another float Tensor\n",
    "# self.long() is equivalent to self.to(torch.int64).\n",
    "F.one_hot(torch.tensor(5), num_classes=27).float() @ C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4087acfa-ccbf-415b-aca0-7e7c3893571f",
   "metadata": {},
   "source": [
    "We get the same embedding as before when we grabbed the 5th index of the Tensor C. \n",
    "\n",
    "Therefore in the lookup table highlighted below, we can retrieve the embedding for a character/word/token either by indexing into the tensor of embeddings or by one-hot encoding the vocabulary and then doing an efficient matrix multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8502fd1a-1143-4c26-a9f7-efa871d2f56d",
   "metadata": {},
   "source": [
    "![asd](neural_probabalistic_language_model_architecture_lookup_table.png \"Character lookup table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853efc63-0b18-447e-abcb-7ae117df25df",
   "metadata": {},
   "source": [
    "Here we will index as it is much faster and use a character table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "902b3680-fa09-46bf-9697-fb8d62db180e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2921,  0.2726],\n",
       "        [-0.0924, -0.9406],\n",
       "        [-0.6139,  0.4221]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also index with a list or tensor\n",
    "C[[5, 6, 7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1ac1071-9f40-4763-becb-1f1e105c768e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2921,  0.2726],\n",
       "        [-0.0924, -0.9406],\n",
       "        [-0.6139,  0.4221],\n",
       "        [-0.6139,  0.4221],\n",
       "        [-0.6139,  0.4221],\n",
       "        [-0.6139,  0.4221],\n",
       "        [-0.6139,  0.4221]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[torch.tensor([5, 6, 7, 7, 7, 7, 7])] # Adding the same index will replicate it multiple times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fbf620-fb6d-4adc-b07a-1efcec58a0c9",
   "metadata": {},
   "source": [
    "We can also index with a multi-dimensional tensor like out input data `X`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1514862a-0ee9-48b5-ae03-2cd025154216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3, 2]),\n",
       " tensor([[[ 1.1212,  0.1038],\n",
       "          [ 1.1212,  0.1038],\n",
       "          [ 1.1212,  0.1038]],\n",
       " \n",
       "         [[ 1.1212,  0.1038],\n",
       "          [ 1.1212,  0.1038],\n",
       "          [ 1.2921,  0.2726]],\n",
       " \n",
       "         [[ 1.1212,  0.1038],\n",
       "          [ 1.2921,  0.2726],\n",
       "          [-2.8325,  1.4847]],\n",
       " \n",
       "         [[ 1.2921,  0.2726],\n",
       "          [-2.8325,  1.4847],\n",
       "          [-2.8325,  1.4847]],\n",
       " \n",
       "         [[-2.8325,  1.4847],\n",
       "          [-2.8325,  1.4847],\n",
       "          [-1.6709,  0.5939]],\n",
       " \n",
       "         [[ 1.1212,  0.1038],\n",
       "          [ 1.1212,  0.1038],\n",
       "          [ 1.1212,  0.1038]],\n",
       " \n",
       "         [[ 1.1212,  0.1038],\n",
       "          [ 1.1212,  0.1038],\n",
       "          [-0.6087,  1.4239]],\n",
       " \n",
       "         [[ 1.1212,  0.1038],\n",
       "          [-0.6087,  1.4239],\n",
       "          [ 0.1451,  2.2464]],\n",
       " \n",
       "         [[-0.6087,  1.4239],\n",
       "          [ 0.1451,  2.2464],\n",
       "          [ 0.8555,  0.2140]],\n",
       " \n",
       "         [[ 0.1451,  2.2464],\n",
       "          [ 0.8555,  0.2140],\n",
       "          [ 0.0269, -0.8091]],\n",
       " \n",
       "         [[ 0.8555,  0.2140],\n",
       "          [ 0.0269, -0.8091],\n",
       "          [ 0.8555,  0.2140]],\n",
       " \n",
       "         [[ 0.0269, -0.8091],\n",
       "          [ 0.8555,  0.2140],\n",
       "          [-1.6709,  0.5939]],\n",
       " \n",
       "         [[ 1.1212,  0.1038],\n",
       "          [ 1.1212,  0.1038],\n",
       "          [ 1.1212,  0.1038]],\n",
       " \n",
       "         [[ 1.1212,  0.1038],\n",
       "          [ 1.1212,  0.1038],\n",
       "          [-1.6709,  0.5939]],\n",
       " \n",
       "         [[ 1.1212,  0.1038],\n",
       "          [-1.6709,  0.5939],\n",
       "          [ 0.0269, -0.8091]],\n",
       " \n",
       "         [[-1.6709,  0.5939],\n",
       "          [ 0.0269, -0.8091],\n",
       "          [-1.6709,  0.5939]],\n",
       " \n",
       "         [[ 1.1212,  0.1038],\n",
       "          [ 1.1212,  0.1038],\n",
       "          [ 1.1212,  0.1038]],\n",
       " \n",
       "         [[ 1.1212,  0.1038],\n",
       "          [ 1.1212,  0.1038],\n",
       "          [ 0.8555,  0.2140]],\n",
       " \n",
       "         [[ 1.1212,  0.1038],\n",
       "          [ 0.8555,  0.2140],\n",
       "          [ 0.0562, -2.2055]],\n",
       " \n",
       "         [[ 0.8555,  0.2140],\n",
       "          [ 0.0562, -2.2055],\n",
       "          [-1.6709,  0.5939]],\n",
       " \n",
       "         [[ 0.0562, -2.2055],\n",
       "          [-1.6709,  0.5939],\n",
       "          [-0.2411, -0.7757]],\n",
       " \n",
       "         [[-1.6709,  0.5939],\n",
       "          [-0.2411, -0.7757],\n",
       "          [ 1.2921,  0.2726]],\n",
       " \n",
       "         [[-0.2411, -0.7757],\n",
       "          [ 1.2921,  0.2726],\n",
       "          [ 0.1451,  2.2464]],\n",
       " \n",
       "         [[ 1.2921,  0.2726],\n",
       "          [ 0.1451,  2.2464],\n",
       "          [ 0.1451,  2.2464]],\n",
       " \n",
       "         [[ 0.1451,  2.2464],\n",
       "          [ 0.1451,  2.2464],\n",
       "          [-1.6709,  0.5939]],\n",
       " \n",
       "         [[ 1.1212,  0.1038],\n",
       "          [ 1.1212,  0.1038],\n",
       "          [ 1.1212,  0.1038]],\n",
       " \n",
       "         [[ 1.1212,  0.1038],\n",
       "          [ 1.1212,  0.1038],\n",
       "          [ 0.0562, -2.2055]],\n",
       " \n",
       "         [[ 1.1212,  0.1038],\n",
       "          [ 0.0562, -2.2055],\n",
       "          [-0.6087,  1.4239]],\n",
       " \n",
       "         [[ 0.0562, -2.2055],\n",
       "          [-0.6087,  1.4239],\n",
       "          [-0.1148, -1.6994]],\n",
       " \n",
       "         [[-0.6087,  1.4239],\n",
       "          [-0.1148, -1.6994],\n",
       "          [-0.8019, -1.5593]],\n",
       " \n",
       "         [[-0.1148, -1.6994],\n",
       "          [-0.8019, -1.5593],\n",
       "          [ 0.8555,  0.2140]],\n",
       " \n",
       "         [[-0.8019, -1.5593],\n",
       "          [ 0.8555,  0.2140],\n",
       "          [-1.6709,  0.5939]]]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[X].shape, C[X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "76d98f69-b311-47ff-ab3e-bcc4653cafee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[13, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2582546e-dfe6-4b1f-9cb9-985bc62da8a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.6709,  0.5939])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[X][13, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0ccd0fe9-d4b1-4ec9-928c-d0c0c81465f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.6709,  0.5939])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b75ca7-b49f-497e-936b-7c4336cf6781",
   "metadata": {},
   "source": [
    "Teherefore we can easily embed all of the input data X by using C[X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a10d162b-c824-4c31-a3b7-ae9c5c794eba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X]\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0793bbb5-59e1-477d-b5e3-e5e981dc6fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = torch.randn((6, 100))\n",
    "b1 = torch.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "11a76ed8-bac4-4985-8cab-e6fa31f303cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below doesn't work since emb has shape 32x3x2 and W1 has shape 6x100\n",
    "# emb @ W1 + b1 \n",
    "# How do we transform this so that it does work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4245d202-9e0c-46c8-bb34-bad19a1e2563",
   "metadata": {},
   "source": [
    "So we now need to concatenate the embeddings for each of the context words together to pass through the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5719705-255e-43a4-b160-b0fbb5c65547",
   "metadata": {},
   "source": [
    "![ads](neural_probabalistic_language_model_architecture_input_embeddings.png \"Input embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7594b97-7911-46fb-a6d4-e0700908fc42",
   "metadata": {},
   "source": [
    "To do this, we can use efficient PyTorch concatenating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "63cbd205-4e26-4e9b-90b1-b04cf182a032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4ba289-0d9b-4c7a-8b41-e9b7701071c4",
   "metadata": {},
   "source": [
    "1st dimension 32 = number of input examples\n",
    "\n",
    "2nd dimension 3 = context size (words in the context of the network)\n",
    "\n",
    "3rd dimension 2 = dimensionality of the embeddings\n",
    "\n",
    "We need to concatenate the 2nd dimension here together so that the final dimension is 32x6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "80c87670-5102-40fb-8df0-524a0b8b3df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([96, 2])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.concat([emb[:, 0, :], emb[:, 1, :], emb[:, 2, :]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5034c6-ef65-4521-8df4-afba4d64f325",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
